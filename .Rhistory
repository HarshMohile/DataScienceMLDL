res1 <- test$Salary - pred$fit
rme_pred_test <- sqrt(mean(res1^2))
rme_pred_test   # 33494.24 for  Actual vs PRedicted
# Muliple Linear regression
library(readr)
sales <- read.csv(file.choose())
View(sales)
View(sales)
View(Cars)
View(sales)
attach(sales)
View(sales)
View(sales)
# Removing the first column which is numerical index 'X'
sales <- sales[:,c(-1)]
# Removing the first column which is numerical index 'X'
sales <- sales[,c(-1)]
View(sales)
View(sales)
names(sales)
# Dummy variable for discrete  categorical data in dataset
sales$cd <- ifelse(sales$cd == 'yes', 1, 0)
sales$cd <- as.factor(sales$cd)
View(sales)
View(sales)
class(sales$cd)
sales$multi <- ifelse(sales$multi == 'yes', 1, 0)
sales$multi <- as.factor(sales$multi)
class(sales$multi)
sales$premium <- ifelse(sales$premium == 'yes', 1, 0)
sales$premium <- as.factor(sales$premium)
class(sales$premium)
qqline(sales$speed)
qqnorm(sales$price)
qqline(sales$price)
summary(sales)
# Scatter plot
plot(sales$price, sales$speed) # Plot relation ships between each X with Y
plot(sales$price, sales$ram)
# Or make a combined plot
pairs(sales)   # Scatter plot for all pairs of variables
# Or make a combined plot
pairs(sales)   # Scatter plot for all pairs of variables
plot(sales)
# The Linear Model of interest
model1 <- lm(price ~ speed + hd + ram + screen + cd + multi + premium + ads + trend, data = sales) # lm(Y ~ X)
summary(model1)
model.speed <- lm(price ~ speed)
summary(model.carV)
summary(model.speed)
model.hd <- lm(price ~ hd)
summary(model.hd)
model.adstrend <- lm(price ~ ads + trend)
summary(model.adstrend)
#### Scatter plot matrix with Correlations inserted in graph
# install.packages("GGally")
library(GGally)
ggpairs(sales)
?corpcor
library(corpcor)
?corpcor
?corpcor
??corpcor
cor(sales)
# Muliple Linear regression
library(readr)
sales <- read.csv(file.choose())
attach(sales)
# Removing the first column which is numerical index 'X'
sales <- sales[,c(-1)]
names(sales)
# Dummy variable for discrete  categorical data in dataset since it has only 2 category Y/N
sales$cd <- ifelse(sales$cd == 'yes', 1, 0)
#sales$cd <- as.factor(sales$cd)
class(sales$cd)
sales$multi <- ifelse(sales$multi == 'yes', 1, 0)
#sales$multi <- as.factor(sales$multi)
class(sales$multi)
sales$premium <- ifelse(sales$premium == 'yes', 1, 0)
#sales$premium <- as.factor(sales$premium)
class(sales$premium)
# The Linear Model of interest
model1 <- lm(price ~ speed + hd + ram + screen + cd + multi + premium + ads + trend, data = sales) # lm(Y ~ X)
summary(model1)
model.speed <- lm(price ~ speed)
summary(model.speed)
model.hd <- lm(price ~ hd)
summary(model.hd)
model.adstrend <- lm(price ~ ads + trend)
summary(model.adstrend)
cor(sales)
cor2pcor(cor(sales))
library(corpcor)
cor2pcor(cor(sales))
which.max(cor(sales))
plot(model1)# Residual Plots, QQ-Plot, Std. Residuals vs Fitted, Cook's distance
qqPlot(model1, id.n = 5) # QQ plots of studentized residuals, helps identify outliers
library(car)
qqPlot(model1, id.n = 5) # QQ plots of studentized residuals, helps identify outliers
# Deletion Diagnostics for identifying influential obseravations
influenceIndexPlot(model1, id.n = 3) # Index Plots of the influence measures
abline(model1,col="red",lwd=2)
plot(model1)# Residual Plots, QQ-Plot, Std. Residuals vs Fitted, Cook's distance
# Deletion Diagnostics for identifying influential obseravations
?influenceIndexPlot
influenceIndexPlot(model1, id.n = 3 ,vars =Bonf) # Index Plots of the influence measures
influenceIndexPlot(model1, id.n = 3 ,vars ="Bonf") # Index Plots of the influence measures
influenceIndexPlot(model1, id.n = 3)
influencePlot(model1, id.n = 3) # A user friendly representation of the above
par(mfrow= c(2,2) )
plot(model1)
qqPlot(model1, id.n = 5) # QQ plots of studentized residuals, helps identify outliers
par(mfrow= c(1)) ## all above plots in  2 by 2  in one plot
# Deletion Diagnostics for identifying influential obseravations
?influenceIndexPlot
influenceIndexPlot(model1, id.n = 3 ,vars ="Bonf") # Index Plots of the influence measures
qqPlot(model1, id.n = 5) # QQ plots of studentized residuals, helps identify outliers
# Regression after deleting the 1441 and 1701 observation row
sales <-  sales[,c(-1441,)]
# Regression after deleting the 1441 and 1701 observation row
sales <-  sales[,c(-1441)]
sales <- sales[,c(-1701)]
model.sales <- lm(price ~ speed + hd + ram + screen + cd + multi + premium + ads + trend, data = sales)
summary(model.sales)
### Variance Inflation Factors
vif(model.sales)  # VIF is > 10 => collinearity
# Regression model to check R^2 on Independent variables
VIFSP <- lm(speed ~ hd + ram + screen + cd + multi + premium + ads + trend +price)
VIFSP
VIFHD <- lm(hd ~  ram + screen + cd + multi + premium + ads + trend +price +speed)
VIFHD
VIFScr <- lm(screen ~ ram  + cd + multi + premium + ads + trend +price +speed + hd)
VIFRAM <- lm(ram ~ cd + multi + premium + ads + trend +price +speed + hd +screen)
summary(VIFSP)
summary(VIFHD)
summary(VIFScr)
summary(VIFRAM)
#### Added Variable Plots ######
avPlots(model.sales, id.n = 2, id.cex = 0.8, col = "red")
# Variance Influence Plot
vif(model.sales)
# Evaluation Model Assumptions
plot(model.sales)
plot(model.sales$fitted.values, model.sales$residuals)
qqnorm(model.sales$residuals)
qqline(model.sales$residuals)
qqnorm(model.sales$residuals)
qqline(model.sales$residuals)
library(leaps)
lm_best <- regsubsets(price ~ ., data = sales, nvmax = 15)
summary(lm_best)
summary(lm_best)$adjr2
summary(lm_best)
lm_forward <- regsubsets(price ~ ., data = sales, nvmax = 15, method = "forward")
summary(lm_forward)
library(leaps)
lm_best <- regsubsets(price ~ ., data = sales, nvmax = 15)
lm_best <- regsubsets(price ~ ., data = sales, nvmax = 15)
?regsubsets
summary(lm_best)$adjr2    # Adjusted R Squared as column name hence $adjr2 off summary of regsubset's model
which.max(summary(lm_best)$adjr2)
coef(lm_best, 3)
lm_forward <- regsubsets(price ~ ., data = sales, nvmax = 15, method = "forward")
summary(lm_forward)
summary(lm_forward)$adjr2    # Adjusted R Squared as column name hence $adjr2 off summary of regsubset's model
which.max(summary(lm_forward)$adjr2)
coef(lm_forward, 3) # top 3 coefficients (y-intercept)
# Backward subset selection
lm_backward <- regsubsets(price ~ ., data = sales, nvmax = 15, method = "backward")
summary(lm_backward)
summary(lm_backward)$adjr2    # Adjusted R Squared as column name hence $adjr2 off summary of regsubset's model
which.max(summary(lm_backward)$adjr2)
coef(lm_backward, 3) # top 3 coefficients (y-intercept) using forward subset selection
library(caTools)
set.seed(0)
split <- sample.split(sales$price, SplitRatio = 0.8)# --------- both are X_train , X_test
train <- subset(sales, split == TRUE)
test <- subset(sales, split == FALSE)
# Model Training
model_final <- lm(price ~ ., data=train)
summary(model_final)
# Predictions
pred <- predict(model_final, newdata = test)
error <- sales$price - pred
pred <- as.dataframe(pred)
pred <- as.data.frame(pred)
error <- sales$price - pred
error
pred
test.rmse <- sqrt(mean(error**2))
test.rmse
train.rmse <- sqrt(mean(model$residuals**2))
train.rmse
test.rmse <- sqrt(mean(error**2))
error
library(MASS)
test.rmse
train.rmse
train.rmse <- sqrt(mean(model$residuals**2))
train.rmse <- sqrt(mean(model_final$residuals**2)) #  other way to just get residual off model
train.rmse
library(MASS)
stepAIC(model_inal)
stepAIC(model_final)
library(readr)
toy <- read.csv(file.choose())
# check or Na or null values
str(toy)
toy.names
names(toy)
toy <- toy[],c("Price","Age_08_04","KM","HP","cc","Doors","Gears","Quarterly_Tax","Weight")]
toy <- toy[,c("Price","Age_08_04","KM","HP","cc","Doors","Gears","Quarterly_Tax","Weight")]
View(toy)
View(toy)
# check or Na or null values
str(toy)
sum(toy)
sum(is.na(toy))
sum(is.null(toy))
qqnorm(toy$Price)
qqnorm(toy$Price)
qqline(toy$Price)
# Pair plot
pairs(toy)
# KM and Age has a corr  and its hetroscedasticity
c <- corr(toy)
# KM and Age has a corr  and its hetroscedasticity
corr(toy)
# KM and Age has a corr  and its hetroscedasticity
cor(toy)
View(toy)
View(toy)
# Splitting into X and y
X =toy[,c(-1)]
View(X)
View(X)
X <- model.matrix(Price ~ ., data = toy)[,-1]
y <- toy$Price
View(X)
View(X)
grid <- 10^seq(10, -2, length = 100)
grid
# Ridge Regression
model_ridge <- glmnet(X, y, alpha = 0, lambda = grid)
# Splitting into X and y
#X =toy[,c(-1)]
#y=toy$Price
library(glmnet)
# Ridge Regression
model_ridge <- glmnet(X, y, alpha = 0, lambda = grid)
summary(model_ridge)
# Using CV
cv_fit <- cv.glmnet(X, y, alpha = 0, lambda = grid)
plot(cv_fit)
seq
optimumlambda <- cv_fit$lambda.min
optimumlambda
y_a <- predict(model_ridge, s = optimumlambda, newx = X)
y_a
pred <- predict(model_ridge, s = optimumlambda, newx = X)
# Model Evaluation
sse <- sum((pred-y)^2)
sst <- sum((y - mean(y))^2)
rsquared <- 1-sse/sst
rsquared
rsquared <- 1-sse/sst
rsquared
#------------------------------------ Lasso Regression --------------------------------------------
# ussing alpha as 1
model_lasso <- glmnet(X, y, alpha = 1, lambda = grid)
summary(model_lasso)
cv_fit_las <- cv.glmnet(x, y, alpha = 1, lambda = grid)
cv_fit_las <- cv.glmnet(X, y, alpha = 1, lambda = grid)
plot(cv_fit_las)
#optimum lambda
optimumlambda_1 <- cv_fit_las$lambda.min
pred_lasso <- predict(model_lasso, s = optimumlambda_1, newx = X)
sse <- sum((pred_lasso-y)^2)
# Model Evaluation
sse <- sum((pred_lasso-y)^2)
sst <- sum((y - mean(y))^2)
rsquared <- 1-sse/sst
rsquared
predict(model_ridge, s = optimumlambda, type="coefficients", newx = x)
predict(model_lasso, s = optimumlambda, type="coefficients", newx = X)
# in LRR glmnet package requires X  into matrix form
library(readr)
strt <- read.csv(file.choose())
strt <- read.csv(file.choose())
View(strt)
View(strt)
View(strt)
View(strt)
strt$State <- as.factor(strt$State)
library(dplyr)
# check or Na or null values
str(strt)
sum(strt) # we are able to add since it has no NA values
View(strt)
View(strt)
qqnorm(strt$Profit)
qqline(strt$Profit)
plot(x = strt$R.D.Spend y =strt$Profit)
plot(x = strt$R.D.Spend ,y =strt$Profit)
# Pair plot
pairs(strt)
# KM and Age has a corr  and its hetroscedasticity
cor(strt)
library(glmnet)
View(strt)
View(strt)
View(strt)
View(strt)
X <- model.matrix(Profit ~ ., data = strt)[,-5]
View(X)
View(X)
View(X)
View(X)
View(X)
View(strt)
View(strt)
# in LRR glmnet package requires X  into matrix form
library(readr)
strt <- read.csv(file.choose())
# Categorical character conversion
#strt$State <- as.factor(strt$State)
strt <- strt::dummy_cols(strt)
# Categorical character conversion
#strt$State <- as.factor(strt$State)
strt <- fastDummies::dummy_cols(strt)
qqnorm(strt$Profit)
qqline(strt$Profit)
# Pair plot
pairs(strt)
library(mlr)
install.packages(mlr)
install.packages("mlr")
library(mlr)
createDummyFeatures(strt, cols = "State")
View(strt)
View(strt)
createDummyFeatures(strt, cols = "strt$State")
createDummyFeatures(strt$State, cols = "State")
View(strt)
st1 <- createDummyFeatures(strt$State, cols = "State")
View(st1)
View(st1)
?cbind
# Logistic regeression On bank data
library(readr)
bank <- read.csv(file.choose())
View(bank)
View(bank)
# Data cleaning
sum(is.na(bank))
# Omitting NA values from the Data
claimants1 <- na.omit(claimants) # na.omit => will omit the rows which has atleast 1 NA value
# Omitting NA values from the Data
bank <- na.omit(bank) # na.omit => will omit the rows which has atleast 1 NA value
dim(bank)
View(bank)
names(bank)
cor(bank$age, bank$balance)
cor(bank) # correlation matrix
# See the proportion
prop.table(table(bank$y))
b1 <-rnorm(bank)
library(caTools)
set.seed(0)
split <- sample.split(bank$y, SplitRatio = 0.8)# --------- both are X_train , X_test
train <- subset(bank, split == TRUE)
test <- subset(bank, split == FALSE)
library(ggplot2)
p1 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p2 <- ggplot(df, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p3 <- ggplot(df, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
p4 <- ggplot(df, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
grid.arrange(p1, p2, p3, p4, ncol=2)
p1 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p2 <- ggplot(df, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p3 <- ggplot(df, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
p4 <- ggplot(df, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
p1 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p2 <- ggplot(df, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
View(bank)
p1 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p2 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p3 <- ggplot(bank, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
p4 <- ggplot(bank, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
grid.arrange(p1, p2, p3, p4, ncol=2)
p1 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p2 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p3 <- ggplot(bank, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
p4 <- ggplot(bank, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
library(car)
library(DescTools)
library(ggplot2)
library(gridExtra)
library(faraway)
library(knitr)
library(performance)
library(ResourceSelection)
library(gridExtra)
p1 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p2 <- ggplot(bank, aes(x=train, y=test)) + geom_point(alpha=0.33) + geom_smooth()
p3 <- ggplot(bank, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
p4 <- ggplot(bank, aes(x=train, group=test, color=test, fill=test)) + geom_density(alpha=0.4)
grid.arrange(p1, p2, p3, p4, ncol=2)
#------------------------------------ model building initial
# Preparing a linear regression
mod_lm <- lm(y ~ ., data = bank)
summary(mod_lm)
View(bank)
View(bank)
pred1 <- predict(mod_lm, bank) # whole bank_data as test
pred1
plot(bank$y, pred1)
model <- glm(y ~ ., data = bank, family = "binomial")  # 0,1
summary(model)
# Log Ratio
exp(coef(model))
# Prediction
pred <- predict(model, bank, type = "response")
pred
# Prediction
pred_prob <- predict(model, bank, type = "response")
pred_prob
# threshold on when to classify Yes or No , 0 or 1
confusion <- table(pred_prob > 0.5, bank$y)
confusion
# Model Accuracy
Acc <- sum(diag(confusion)/sum(confusion))
Acc
# Convert the probabilities to binary output form using cutoff
pred_values <- ifelse(prob > 0.5, 1, 0)
# Convert the probabilities to binary output form using cutoff
pred_values <- ifelse(prob > 0.5, 1, 0)
# Convert the probabilities to binary output form using cutoff
pred_values <- ifelse(pred_prob > 0.5, 1, 0)
pred_values
library(caret)
# Confusion Matrix
confusionMatrix(factor(bank$y, levels = c(0, 1)), factor(pred_values, levels = c(0, 1)))
library(readr)
#install.packages("dummies")
library(dummies)
library(ggplot2)
library(dplyr)
library(caret)
setwd('D:\\360Assignments\\Submission')
# Load the Dataset
ff <- read.csv(file.choose())
View(ff)
View(ff)
ff <- ff[3:]
ff <- ff[:,c(-1,-2)]
ff <- ff[,c(-1,-2)]
View(ff)
View(ff)
ff$size_category <- factor(ff$size_category)
# Splitting the train test
library(caTools)
set.seed(0)
split <- sample.split(ff$size_category, SplitRatio = 0.8)# --------- both are X_train , X_test
train <- subset(ff, split == TRUE)
test <- subset(ff, split == FALSE)
svm.model <- svm(size_category ~ .,data=train ,
type='C-classification',
kernel='linear',
scale=FALSE)
library(e1071)
svm.model <- svm(size_category ~ .,data=train ,
type='C-classification',
kernel='linear',
scale=FALSE)
summary()
summary(svm.model)
# get parameter of the hyperplane
w <- t(svm.model$coefs) %*% svm.model$SV  # weights
b <- svm.model$rho   # negative intercepts
## Evaluating model performance ----
# predictions on testing dataset
predictions <- predict(svm.model, test)
table(predictions, ff$size_category)
table(predictions, test$size_category)
accuracy <- predictions == test$size_category
table(accuracy)
prop.table(table(accuracy))
svm.radial <- svm(Salary ~ .,data=train ,
type='C-classification',
kernel='radial',
scale=FALSE)
svm.radial <- svm(size_category ~ .,data=train ,
type='C-classification',
kernel='radial',
scale=FALSE)
summary(svm.radial)
# get parameter of the hyperplane
w <- t(svm.radial$coefs) %*% svm.radial$SV  # weights
b <- svm.radial$rho   # negative intercepts
## Evaluating model performance ----
# predictions on testing dataset
predictions <- predict(svm.radial, test$size_category)
## Evaluating model performance ----
# predictions on testing dataset
predictions <- predict(svm.radial, test)
table(predictions, test$size_category)
accuracy <- predictions == test$size_category
table(accuracy)
b
prop.table(table(accuracy))
prop.table(table(accuracy))
